{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4ecade0-7ba9-4b2c-8b00-f1221fde4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4af102cb-addd-43c9-a39b-d7abd3119b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/06 08:40:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://85df9d922b1c:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AzureADLS</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fea04b676a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, regexp_replace, split\n",
    "from pyspark.sql import functions as F\n",
    "# spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n",
    "#spark.sql(\"SET spark.sql.catalog.my_catalog.uri=http://192.168.56.1:8181\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "storage_account = os.getenv(\"AZURE_STORAGE_ACCOUNT\")\n",
    "adls_client_id = os.getenv(\"ADLS_CLIENT_ID\")\n",
    "adls_client_secret = os.getenv(\"ADLS_CLIENT_SECRET\")\n",
    "adls_tenant_id = os.getenv(\"ADLS_TENANT_ID\")\n",
    "value = os.getenv('AZURE_DATA_LAKE_CONNECTION_STRING')\n",
    "container_name = os.getenv('AZURE_CONTAINER_NAME')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AzureADLS\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-azure-3.3.2.jar,/opt/spark/jars/hadoop-azure-datalake-3.3.2.jar,/opt/spark/jars/azure-identity-1.14.2.jar,/opt/spark/jars/mssql-jdbc-12.8.1.jre11.jar,/opt/spark/jars/msal4j-1.18.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\") \\\n",
    "    .config(f\"spark.hadoop.fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\") \\\n",
    "    .config(f\"spark.hadoop.fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n",
    "    .config(f\"spark.hadoop.fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", f\"{adls_client_id}\") \\\n",
    "    .config(f\"spark.hadoop.fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", f\"{adls_client_secret}\") \\\n",
    "    .config(f\"spark.hadoop.fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{adls_tenant_id}/oauth2/token\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.fs.azure.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "spark.conf.set(\"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\")\n",
    "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n",
    "spark.conf.set(\"fs.azure.impl.disable.cache\", \"true\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e260eee-8630-450a-9248-e823a109295d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (12.24.0)\n",
      "Requirement already satisfied: azure-identity in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.19.0)\n",
      "Requirement already satisfied: pyspark in /opt/spark/python (from -r requirements.txt (line 4)) (3.5.1)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.9/site-packages (from azure-storage-blob->-r requirements.txt (line 2)) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.9/site-packages (from azure-storage-blob->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.9/site-packages (from azure-storage-blob->-r requirements.txt (line 2)) (44.0.0)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.9/site-packages (from azure-storage-blob->-r requirements.txt (line 2)) (1.32.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.9/site-packages (from azure-identity->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.9/site-packages (from azure-identity->-r requirements.txt (line 3)) (1.31.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/site-packages (from pyspark->-r requirements.txt (line 4)) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/site-packages (from pyspark->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/site-packages (from pyspark->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /usr/local/lib/python3.9/site-packages (from pyspark->-r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.9/site-packages (from azure-core>=1.30.0->azure-storage-blob->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.9/site-packages (from azure-core>=1.30.0->azure-storage-blob->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/site-packages (from cryptography>=2.1.4->azure-storage-blob->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from msal>=1.30.0->azure-identity->-r requirements.txt (line 3)) (2.10.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /usr/local/lib/python3.9/site-packages (from msal-extensions>=1.2.0->azure-identity->-r requirements.txt (line 3)) (2.10.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob->-r requirements.txt (line 2)) (3.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131cad13-8e15-4a5f-ac40-97744e1d03a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iceberg-metadata', 'myviseken-data-lake']\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Test credentials\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id= adls_tenant_id,\n",
    "    client_id= adls_client_id,\n",
    "    client_secret= adls_client_secret\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "service_client = BlobServiceClient(\n",
    "    account_url=\"https://myvisekendatalake.blob.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# List containers to verify connectivity\n",
    "print([container.name for container in service_client.list_containers()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c99fdcf2-9e50-4f79-8672-44821a67c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.io.retry.max.retries\", \"20\")  # Increase retry count\n",
    "spark.conf.set(\"fs.azure.io.retry.backoff\", \"5000\")   # Increase backoff time in ms\n",
    "spark.conf.set(\"fs.azure.io.retry.max.attempts\", \"20\") # Max retry attempts\n",
    "spark.conf.set(\"fs.azure.dfs.socket.timeout\", \"30000\") # 30 seconds socket timeout\n",
    "spark.conf.set(\"fs.azure.read.request.timeout\", \"120000\") # 120 seconds read request timeout\n",
    "spark.conf.set(\"fs.azure.io.read.max.range.size\", \"67108864\")  # Default: 64MB\n",
    "spark.conf.set(\"fs.azure.io.read.range.buffer.size\", \"10485760\")  # Default: 10MB\n",
    "spark.conf.set(\"fs.azure.io.read.range.buffer.timeout\", \"60000\")  # Default: 60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b5fd48-3266-404c-9a22-792ea0bd7c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+----+-------+-------+------------+--------------+------------+--------------------+------+--------------------+--------------------+\n",
      "|listing_id|               title| installment|year|variant|mileage|transmission|      location|       state|       listing_image| price|                 url|               image|\n",
      "+----------+--------------------+------------+----+-------+-------+------------+--------------+------------+--------------------+------+--------------------+--------------------+\n",
      "|  15774123|2012 Perodua Myvi...|RM 214/month|2012|     SE|  77500|   Automatic|Seri Kembangan|    Selangor|https://img1.icar...|   328|https://www.carli...|['https://img1.ic...|\n",
      "|  15964938|2014 Perodua Myvi...|RM 249/month|2014|     SE|  57500|   Automatic|Seri Kembangan|    Selangor|https://img1.icar...|   289|https://www.carli...|['https://img1.ic...|\n",
      "|  15531831|2018 Perodua Myvi...|RM 555/month|2018|     AV|  80118|   Automatic|   Setiawangsa|Kuala Lumpur|https://img1.icar...|42,800|https://www.carli...|['https://img1.ic...|\n",
      "|  15642030|2019 Perodua Myvi...|RM 576/month|2019|      H|  57500|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|44,400|https://www.carli...|['https://img1.ic...|\n",
      "|  15678245|2018 Perodua Myvi...|RM 511/month|2018|      X|  85519|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|39,400|https://www.carli...|['https://img1.ic...|\n",
      "|  15706649|2018 Perodua Myvi...|RM 542/month|2018|     AV|  86732|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|41,800|https://www.carli...|['https://img1.ic...|\n",
      "|  15678228|2017 Perodua Myvi...|RM 478/month|2017|     SE| 108457|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|36,900|https://www.carli...|['https://img1.ic...|\n",
      "|  16051941|2010 Perodua Myvi...|RM 127/month|2010|    SXi| 117500|      Manual|  Ampang Hilir|Kuala Lumpur|https://img1.icar...| 9,800|https://www.carli...|['https://img1.ic...|\n",
      "|  15924927|2018 Perodua Myvi...|RM 478/month|2018|      G|  80458|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|36,900|https://www.carli...|['https://img1.ic...|\n",
      "|  15924732|2018 Perodua Myvi...|RM 542/month|2018|     AV| 109597|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|41,800|https://www.carli...|['https://img1.ic...|\n",
      "|  15924827|2018 Perodua Myvi...|RM 568/month|2018|     AV|  80266|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|43,800|https://www.carli...|['https://img1.ic...|\n",
      "|  15924794|2018 Perodua Myvi...|RM 537/month|2018|      H|  79092|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|41,400|https://www.carli...|['https://img1.ic...|\n",
      "|  15924756|2019 Perodua Myvi...|RM 560/month|2019|      H|  93940|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|43,200|https://www.carli...|['https://img1.ic...|\n",
      "|  15924911|2020 Perodua Myvi...|RM 595/month|2020|      H|  44879|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|45,900|https://www.carli...|['https://img1.ic...|\n",
      "|  15924708|2019 Perodua Myvi...|RM 594/month|2019|     AV|  86818|   Automatic| Petaling Jaya|    Selangor|https://img1.icar...|45,800|https://www.carli...|['https://img1.ic...|\n",
      "|  16050522|2015 Perodua Myvi...|RM 336/month|2015|      X| 156800|   Automatic|         Masai|       Johor|https://img1.icar...|25,900|https://www.carli...|['https://img1.ic...|\n",
      "|  16050498|2023 Perodua Myvi...|RM 752/month|2023|     AV|  18059|   Automatic|   Setiawangsa|Kuala Lumpur|https://img1.icar...|58,000|https://www.carli...|['https://img1.ic...|\n",
      "|  16050477|2020 Perodua Myvi...|RM 634/month|2020|     AV|  45857|   Automatic|         Perai|      Penang|https://img1.icar...|48,900|https://www.carli...|['https://img1.ic...|\n",
      "|  15978720|2019 Perodua Myvi...|RM 605/month|2019|     AV|  62500|   Automatic|   Bayan Lepas|      Penang|https://img1.icar...|46,700|https://www.carli...|['https://img1.ic...|\n",
      "|  15952526|2017 Perodua Myvi...|RM 491/month|2017|     SE|  97500|   Automatic|   Bayan Lepas|      Penang|https://img1.icar...|37,900|https://www.carli...|['https://img1.ic...|\n",
      "+----------+--------------------+------------+----+-------+-------+------------+--------------+------------+--------------------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "# spark.conf.set(\"fs.azure.debug.logging\", \"true\")\n",
    "csv_file = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/raw/carlist/data_20250102170445.csv\"\n",
    "csv_test_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/raw/carlist/data_20250102170445.csv\"\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"listing_id\", IntegerType(), True),\n",
    "#     StructField(\"title\", StringType(), True),\n",
    "#     StructField(\"installment\", StringType(), True),\n",
    "#     StructField(\"year\", IntegerType(), True),\n",
    "#     StructField(\"variant\", StringType(), True),\n",
    "#     StructField(\"mileage\", IntegerType(), True),\n",
    "#     StructField(\"transmission\", StringType(), True),\n",
    "#     StructField(\"location\", StringType(), True),\n",
    "#     StructField(\"state\", StringType(), True),\n",
    "#     StructField(\"listing_image\", StringType(), True),\n",
    "#     StructField(\"price\", StringType(), True),\n",
    "#     StructField(\"url\", StringType(), True),\n",
    "#     StructField(\"image\", StringType(), True) \n",
    "# ])\n",
    "\n",
    "# # # Read the CSV\n",
    "df = spark.read.option(\"multiline\", \"true\").csv(csv_test_path, header=True)\n",
    "# df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file)\n",
    "# # Read the file as text\n",
    "# # raw_data = spark.read.text(csv_test_path)\n",
    "\n",
    "# # # Manually process each line to handle newline in the last column\n",
    "# # processed_data = raw_data.rdd.map(lambda x: x[0].replace(\"\\n\", \" \"))\n",
    "\n",
    "# # # Convert back to DataFrame\n",
    "# # df = spark.read.csv(processed_data)\n",
    "\n",
    "# # Show the result\n",
    "# df.show()\n",
    "\n",
    "# # Show the first few rows\n",
    "df.show()\n",
    "# print(csv_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38fbb95c-1b20-4bdb-b9a7-7442d06b9ef5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o453.jdbc.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Failed to load MSAL4J Java library for performing ActiveDirectoryPassword authentication.\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.getFedAuthToken(SQLServerConnection.java:6019)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.onFedAuthInfo(SQLServerConnection.java:5995)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.processFedAuthInfo(SQLServerConnection.java:5829)\n\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onFedAuthInfo(tdsparser.java:335)\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:130)\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:42)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:6888)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:5434)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:5366)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4391)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3828)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3385)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:3194)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1971)\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1263)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m jdbc_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:sqlserver://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.database.windows.net:1433;database=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatabase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;user=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;password=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m jdbc_url_aad \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:sqlserver://myviseken.database.windows.net:1433;database=myviseken-db;user=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maad_user\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;password=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maad_password\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;authentication=ActiveDirectoryPassword\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjdbc_url_aad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maad_user\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maad_password\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(jdbc_url)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:946\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    944\u001b[0m     jpredicates \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtoJArray(gateway, gateway\u001b[38;5;241m.\u001b[39mjvm\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mString, predicates)\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o453.jdbc.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Failed to load MSAL4J Java library for performing ActiveDirectoryPassword authentication.\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.getFedAuthToken(SQLServerConnection.java:6019)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.onFedAuthInfo(SQLServerConnection.java:5995)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.processFedAuthInfo(SQLServerConnection.java:5829)\n\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onFedAuthInfo(tdsparser.java:335)\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:130)\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:42)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:6888)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:5434)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:5366)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4391)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3828)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3385)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:3194)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1971)\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1263)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# connecting spark server to the azure sql database\n",
    "\n",
    "server_name = os.getenv(\"SQL_SERVER\")\n",
    "database_name = os.getenv(\"DB_NAME\")\n",
    "username = os.getenv(\"DB_USERNAME\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "aad_user = os.getenv(\"AAD_USER\")\n",
    "aad_password = os.getenv(\"AAD_PASSWORD\")\n",
    "\n",
    "# jdbc connection string \n",
    "jdbc_url = f\"jdbc:sqlserver://{server_name}.database.windows.net:1433;database={database_name};user={username}@{server_name};password={password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "jdbc_url_aad = f\"jdbc:sqlserver://myviseken.database.windows.net:1433;database=myviseken-db;user={aad_user};password={aad_password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;authentication=ActiveDirectoryPassword\"\n",
    "df = spark.read.jdbc(jdbc_url_aad, \"test_table\", properties={\n",
    "    \"user\": f\"{aad_user}\",\n",
    "    \"password\": f\"{aad_password}\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "})\n",
    "df.show()\n",
    "# print(jdbc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3615906-7ff3-4a4b-8714-9e0d263c1e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:java.sql.DriverManager.getConnection. Trace:\npy4j.Py4JException: Method getConnection([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)\n\tat py4j.Gateway.invoke(Gateway.java:276)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 33\u001b[0m\n\u001b[1;32m     18\u001b[0m check_and_create_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mIF NOT EXISTS (\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    SELECT 1\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124mEND;\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Execute the query\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDriverManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m stmt \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreateStatement()\n\u001b[1;32m     35\u001b[0m stmt\u001b[38;5;241m.\u001b[39mexecute(check_and_create_query)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:java.sql.DriverManager.getConnection. Trace:\npy4j.Py4JException: Method getConnection([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)\n\tat py4j.Gateway.invoke(Gateway.java:276)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n"
     ]
    }
   ],
   "source": [
    "# connecting spark server to the azure sql database\n",
    "\n",
    "server_name = os.getenv(\"SQL_SERVER\")\n",
    "database_name = os.getenv(\"SQL_DB\")\n",
    "username = os.getenv(\"DB_USERNAME\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# jdbc connection string \n",
    "jdbc_url = f\"jdbc:sqlserver://{server_name}.database.windows.net:1433;database={database_name};user={username}@{server_name};password={password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": f\"{username}\",\n",
    "    \"password\": f\"{password}\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# SQL query to check and create table\n",
    "check_and_create_query = \"\"\"\n",
    "IF NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM sys.tables\n",
    "    WHERE name = 'test_table'\n",
    ")\n",
    "BEGIN\n",
    "    CREATE TABLE test_schema_name.test_table (\n",
    "        column1 INT,\n",
    "        column2 NVARCHAR(50),\n",
    "        column3 DATETIME\n",
    "    );\n",
    "END;\n",
    "\"\"\"\n",
    "# Execute the query\n",
    "connection = spark._sc._gateway.jvm.java.sql.DriverManager.getConnection(jdbc_url_aad, properties[\"user\"], properties[\"password\"],properties[\"driver\"])\n",
    "stmt = connection.createStatement()\n",
    "stmt.execute(check_and_create_query)\n",
    "stmt.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af0f988e-345a-4c3a-8050-140e2087a5aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SPARK_CLASSPATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSPARK_CLASSPATH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SPARK_CLASSPATH'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_CLASSPATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7ab2b-0fe1-40ff-91d0-44a416440790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b339d813-b78f-49b7-8100-0ca0a689707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   listing_id                                              title  \\\n",
      "0    15774123  2012 Perodua Myvi 1.5 SE Hatchback (MONTHLY RM...   \n",
      "1    15964938  2014 Perodua Myvi 1.3 SE Hatchback (MUKA RM289...   \n",
      "2    15531831                    2018 Perodua Myvi 1.5 Hatchback   \n",
      "3    15642030                  2019 Perodua Myvi 1.5 H Hatchback   \n",
      "4    15678245                  2018 Perodua Myvi 1.3 X Hatchback   \n",
      "\n",
      "    installment  year variant  mileage transmission        location  \\\n",
      "0  RM 214/month  2012      SE    77500    Automatic  Seri Kembangan   \n",
      "1  RM 249/month  2014      SE    57500    Automatic  Seri Kembangan   \n",
      "2  RM 555/month  2018      AV    80118    Automatic     Setiawangsa   \n",
      "3  RM 576/month  2019       H    57500    Automatic   Petaling Jaya   \n",
      "4  RM 511/month  2018       X    85519    Automatic   Petaling Jaya   \n",
      "\n",
      "          state                                      listing_image   price  \\\n",
      "0      Selangor  https://img1.icarcdn.com/32147751/main-m_used-...     328   \n",
      "1      Selangor  https://img1.icarcdn.com/83946951/main-m_used-...     289   \n",
      "2  Kuala Lumpur  https://img1.icarcdn.com/13813551/main-m_used-...  42,800   \n",
      "3      Selangor  https://img1.icarcdn.com/03024651/main-m_used-...  44,400   \n",
      "4      Selangor  https://img1.icarcdn.com/54287651/main-m_used-...  39,400   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.carlist.my/used-cars/2012-perodua-...   \n",
      "1  https://www.carlist.my/used-cars/2014-perodua-...   \n",
      "2  https://www.carlist.my/used-cars/2018-perodua-...   \n",
      "3  https://www.carlist.my/used-cars/2019-perodua-...   \n",
      "4  https://www.carlist.my/used-cars/2018-perodua-...   \n",
      "\n",
      "                                               image  \n",
      "0  ['https://img1.icarcdn.com/32147751/thumb-l_us...  \n",
      "1  ['https://img1.icarcdn.com/83946951/thumb-l_us...  \n",
      "2  ['https://img1.icarcdn.com/13813551/thumb-l_us...  \n",
      "3  ['https://img1.icarcdn.com/03024651/thumb-l_us...  \n",
      "4  ['https://img1.icarcdn.com/54287651/thumb-l_us...  \n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(value)\n",
    "\n",
    "# Access the container and blob\n",
    "container_name = os.getenv('AZURE_CONTAINER_NAME')\n",
    "blob_name = \"raw/carlist/data_20250102170445.csv\"\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "# Download the blob content\n",
    "stream = blob_client.download_blob().readall()\n",
    "\n",
    "# Load into pandas DataFrame\n",
    "import io\n",
    "df = pd.read_csv(io.BytesIO(stream))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca9a09c-fdc8-43e8-a867-a9e8c261aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 09:28:45 WARN TaskSetManager: Stage 4 contains a task of very large size (1775 KiB). The maximum recommended task size is 1000 KiB.\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o165.count",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m myvi_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df)\n\u001b[0;32m----> 2\u001b[0m myvi_df \u001b[38;5;241m=\u001b[39m \u001b[43mmyvi_df\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlisting_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# .orderBy(F.col('year')).desc() \\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# .show()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# myvi_df.count()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o165.count"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "myvi_df = spark.createDataFrame(df)\n",
    "myvi_df = myvi_df \\\n",
    "        .select('listing_id', 'year', 'state', 'price','url') \\\n",
    "        .count()\n",
    "        # .orderBy(F.col('year')).desc() \\\n",
    "        # .show()\n",
    "# myvi_df.count()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
